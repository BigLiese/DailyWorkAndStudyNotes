1. 看算法图解到 38 页
   Notes：
   #ARRAY
   1> "Earth-shattering" is an idiomatic expression used to describe something that is extremely important or significant, often implying that it has a large impact or influence. It suggests that something is so significant that it could cause the earth to shake or tremble. For example, "The discovery of a cure for cancer would be an earth-shattering breakthrough in the medical field."

   2> "Flaky" is an adjective used to describe something that is unreliable or inconsistent. It can be used to describe a person, product, or process that fails to work as expected or consistently produces unpredictable or varying results. For example, "The software has been acting flaky lately, crashing unexpectedly and producing inconsistent results.

3> 4 basic ways of using data structures:

- Read
- Search
- Insert
- Delete

> when we measure how “fast” an operation takes, we do not refer to how fast the operation takes in terms of pure time, but instead in how many steps it takes.

1. A computer can jump to any memory address in one step. (Think of this as driving to 123 Main Street—you can drive there in one trip since you know exactly where it is.)
2. Recorded in each array is the memory address which it begins at. So the computer has this starting address readily.
3. Every array begins at index 0.

--> Reading from an array actually takes just _one step_. This is because the computer has the ability to jump to any particular index in the array and peer inside.

--> search operation—in which the computer checks each cell one at a time—is known as _linear search_.

For N cells in an array, linear search will take a maximum of N steps.

--> insertion in a worst-case scenario can take _up to N + 1 steps_ for an array containing N elements. This is because the worst-case scenario is inserting a value into the beginning of the array in which there are N shifts (every data element of the array) and one insertion.

--> delete for an array containing N elements, the maximum number of steps that deletion would take is _N steps_.

---

#SET
A set is a data structure that does not allow duplicate values to be contained within it.

--> Reading from a set is exactly the same as reading from an array—it takes just one step for the computer to look up what is contained within a particular index

-->Searching a set also turns out to be no different than searching an array—it takes up to _N steps_ to search to see if a value exists within a set.

--> Insertion, insertion into a set in a best-case scenario will take _N + 1 steps_ for N elements. This is because there are N steps of search to ensure that the value doesn’t already exist within the set, and then one step for the actual insertion.

In a worst-case scenario, where we’re inserting a value at the beginning of a set, the computer needs to search N cells to ensure that the set doesn’t already contain that value, and then another N steps to shift all the data to the right, and another final step to insert the new value. That’s a total of _2N + 1 steps_.

-->deletion is also identical between a set and an array—it takes up to _N steps_ to delete a value and move data to the left to close the gap.

---

**Chapter 2 Why Algorithms Matter**

An algorithm is simply a particular process for solving a problem.

> Ordered Arrays: the values of the array remain sorted.

when inserting into an ordered array, we need to always conduct _a search before the actual insertion_ to determine the correct spot for the insertion. That is one key difference (in terms of efficiency) between a standard array and an ordered array.

The major advantage of an ordered array over a standard array is that we have the option of performing a binary search rather than a linear search.

> Binary Search: find the midpoint between the upper and lower bounds

First, we establish the lower and upper bounds of where the value we're searching for can be. To start, the lower bound is the first value in the array, while the upper bound is the last value.

When we use binary search, however, each guess we make eliminates half of the possible cells we’d have to search. In our very first guess, we get to eliminate a whopping fifty cells.

[[binary search.png]]

for every time we double the data, the binary search algorithm adds a maximum of just one more step.

> Binary Search vs. Linear Search

With an array containing one hundred values, here are the maximum numbers of steps it would take for each type of search: Linear search: one hundred steps Binary search: seven steps

[[Binary Search vs. Linear Search.png]]

---

**Chapter 3 Oh Yes! Big O Notation：Count the Steps**

Instead of focusing on units of time, Big O achieves consistency by focusing only on the number of steps that an algorithm takes.

Reading from an array takes just one step, no matter how large the array is. The way to express this in Big O Notation is: O(1)

Let’s examine how Big O Notation would describe the efficiency of linear search：
In a worst-case scenario, linear search will take as many steps as there are elements in the array.

The appropriate way to express this in Big O Notation is：O(N) [pronounced as “Oh of N.”]

Big O answers the following question: how does the number of steps change as the data increases?

> Constant Time vs. Linear Time

[[Constant Time vs. Linear Time.png]]

Even though the algorithm technically takes three steps rather than one step, Big O Notation considers that trivial. O(1) is the way to describe any algorithm that doesn’t change its number of steps even when the data increases.

O(N) is considered to be, on the whole, less efficient than O(1).

> Binary search in Big O

In Big O, we describe binary search as having a time complexity of: O(log N)
[pronounced as “Oh of log N.”]
[Log is shorthand for logarithm.]
[Whenever we say O(log N), it’s actually shorthand for saying O(log2 N). We’re just omitting that small 2 for convenience.]

O(log N) is the Big O way of describing an algorithm that increases one step each time the data is doubled.

[[O(logN) O(1) O(N).png]]

[[Logarithms.png]]

---

**Chapter 4 Speeding Up Your Code with Big O**

> Bubble Sort

Given an array of unsorted numbers, how can we sort them so that they end up in ascending order?

If we get through an entire passthrough without having to make any swaps, we’ll know that the array is completely sorted.

The Bubble Sort algorithm contains two kinds of steps:

Comparisons: two numbers are compared with one another to determine which is greater.
Swaps: two numbers are swapped with one another in order to sort them.

For N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.

In Big O Notation, we would say that Bubble Sort has an efficiency of _O(N2 )_.
In an O(N2 ) algorithm, for N data elements, there are roughly _N2 steps._

[[O(N^2).png]]

[O(N2 ) is also referred to as quadratic time]

A common example of an algorithm with quadratic time complexity is a *nested loop*

---

**Chapter5 Optimizing Code with and Without Big O**

> **Selection Sort**

Selection Sort contains two types of steps: comparisons and swaps. That is, we compare each element with the lowest number we’ve encountered in each passthrough, and we swap the lowest number into its correct position.

The steps of Selection Sort are as follows:

We check each cell of the array from left to right to determine which value is least. As we move from cell to cell, we keep in a variable the lowest value we’ve encountered so far.

If we encounter a cell that contains a value that is even less than the one in our variable, we replace it so that the variable now points to the new index.

Once we’ve determined which index contains the lowest value, we swap that index with the value we began the passthrough with. This would be index 0 in the first passthrough, index 1 in the second passthrough, and so on and so forth.

![[Selection Sort.png]]

> **The Efficiency of Selection Sort**

[[The Efficiency of Selection Sort.png]]

Selection Sort contains about half the number of steps that Bubble Sort does, indicating that Selection Sort is twice as fast.

we’d say that for N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.

As for swaps we only need to make a maximum of one swap per passthrough. This is because in each passthrough, we make either one or zero swaps, depending on whether the lowest number of that passthrough is already in the correct position. Contrast this with Bubble Sort, where in a worst-case scenario—an array in descending order—we have to make a swap for each and every comparison.

> **Ignoring Constants**

In reality, Selection Sort is described in Big O as O(N2 ), just like Bubble Sort.

This is because of a major rule of Big O that we’re now introducing for the first time:
_Big O Notation ignores constants._

> **The Role of Big O**

there really is no such thing as O(100N)—it is simply written as O(N).
Similarly, with large amounts of data, O(log N) will always be faster than O(N), even if the given O(log N) algorithm is actually O(2 \* log N) under the hood.

So Big O is an extremely useful tool, because if two algorithms fall under different classifications of Big O, you’ll generally know which algorithm to use since with large amounts of data, one algorithm is guaranteed to be faster than the other at a certain point.

---

**Chapter 6 Optimizing for Optimistic Scenarios**

> **Insertion Sort**

Insertion Sort consists of the following steps:
![[Insertion Sort.png]]
![[Insertion Sort2.png]]

**The Efficiency of Insertion Sort**

There are four types of steps that occur in Insertion Sort: removals, comparisons, shifts, and insertions. To analyze the efficiency of Insertion Sort, we need to tally up each of these steps.

We can, formulate the total number of comparisons as: 1 + 2 + 3 + … + N - 1 comparisons.

When examining this pattern, it emerges that for an array containing N elements, there are approximately N2 / 2 comparisons. (10 2 / 2 is 50, and 20 2 / 2 is 200.)

Let’s add up comparisons and shifts for a worst-case scenario:
N2 / 2 comparisons + N2 / 2 shifts

Removing and inserting the temp_value from the array happen once per passthrough.
N2 comparisons & shifts combined
N - 1 removals + N - 1 insertions

total is _N2 + 2N - 2 steps_
actual is O(N2)

there is another major rule of Big O :
Big O Notation only takes into account the highest order of N.

if we have some algorithm that takes N4 + N3 + N2 + N steps, we only consider N4 to be significant—and just call it O(N4 ).

Why? As N increases, N4 becomes so much more significant than any other order of N. When N is 1,000, N4 is 1,000 times greater than N3 . Because of this, we only consider the greatest order of N.

We noted in the previous chapter that although Bubble Sort and Selection Sort are both O(N2 ), Selection Sort is faster since Selection Sort has N2 / 2 steps compared with Bubble Sort’s N2 steps. At first glance, then, we’d say that Insertion Sort is as slow as Bubble Sort, since it too has N2 steps. (It’s really N2 + 2N - 2 steps.)

> **The Average Case**

The array [1, 2, 3, 4] is already presorted, which is the best case. The worst case for the same data would be [4, 3, 2, 1], and an example of an average case might be [1, 3, 4, 2].

We can now see that the performance of Insertion Sort varies greatly based on the scenario. In the worst-case scenario, Insertion Sort takes N2 steps.
In an average scenario, it takes N2 / 2 steps.
And in the best-case scenario, it takes about N steps.

Selection Sort takes N2 / 2 steps in all cases, from worst to average to best-case scenarios. This is because Selection Sort doesn’t have any mechanism for ending a passthrough early at any point. Each passthrough compares every value to the right of the chosen index no matter what.

So which is better: Selection Sort or Insertion Sort? The answer is: well, it depends. In an average case—where an array is randomly sorted—they perform similarly. If you have reason to assume that you’ll be dealing with data that is mostly sorted, Insertion Sort will be a better choice. If you have reason to assume that you’ll be dealing with data that is mostly sorted in reverse order, Selection Sort will be faster. If you have no idea what the data will be like, that’s essentially an average case, and both will be equal.

> **A Practical Example**

For example, if you have the arrays [3, 1, 4, 2] and [4, 5, 3, 6], the intersection would be a third array, [3, 4], since both of those values are common to the two arrays.

```
function intersection(first_array, second_array){
       var result = [];
       for (var i = 0; i < first_array.length; i++) {
          for (var j = 0; j < second_array.length; j++) {
             if (first_array[i] == second_array[j]) {
                result.push(first_array[i]);
                break;  //as soon as we find a common value we cut the inner loop short, and save steps
                }
             }
         }
              return result;
    }
```

---

**Chapter 7 Blazing Fast Lookup with Hash Tables**

> **Enter the Hash Table : fast reading**

Other names include hashes, maps, hash maps, dictionaries, and associative arrays, Object.

**A hash table is a list of paired values. **
The first item is known as the key, and the second item is known as the value.

Looking up a value in a hash table has an efficiency of O(1) on average, as it takes just one step.

> **Dealing with Collisions**

Trying to add data to a cell that is already filled is known as a collision.

One classic approach for handling collisions is known as separate chaining. When a collision occurs, instead of placing a single value in the cell, it places in it a reference to an array.

[[Dealing with Collisions.png]]

In a scenario where the computer hits upon a cell that references an array, its search can take some extra steps, as it needs to conduct a linear search within an array of multiple values. If somehow all of our data ended up within a single cell of our hash table, our hash table would be no better than an array.

So it actually turns out that the worst-case performance for a hash table lookup is O(N). Because of this, it is critical that a hash table be designed in a way that it will have few collisions, and therefore typically perform lookups in O(1) time rather than O(N) time.

> **The Great Balancing Act**

Ultimately, a hash table’s efficiency depends on three factors:

- How much data we’re storing in the hash table
- How many cells are available in the hash table
- Which hash function we’re using

_A good hash table strikes a balance of avoiding collisions while not consuming lots of memory._

Computer scientists have developed the following rule of thumb: for every seven data elements stored in a hash table, it should have ten cells.

This ratio of data to cells is called the load factor. Using this terminology, we’d say that the ideal load factor is 0.7 (7 elements / 10 cells).

```
function hasDuplicateValue(array) {
      var existingValues = {};
      for(var i = 0; i < array.length; i++) {
         if(existingValues[array[i]] === undefined) {
               existingValues[array[i]] = 1;
               } else {
               return true;
             }
        }
        return false；
    }
```

The truth is that hash tables are perfect for any situation where we want to keep track of which values exist within a dataset.

---

**Chapter 8 Crafting Elegant Code with Stacks and Queues**

> \*\*data structures: stacks and queues --- arrays with restrictions

Stacks and queues are elegant tools for handling temporary data

Temporary data is information that doesn’t have any meaning after it’s processed, so you don’t need to keep it around. However, the order in which you process the data can be important.

Stacks and queues allow you to handle data in order, and then get rid of it once you don’t need it anymore.

> **Stacks**

A stack stores data in the same way that arrays do—it’s simply a list of elements.
The one catch is that stacks have the following three constraints:

- Data can only be inserted at the end of a stack.
- Data can only be read from the end of a stack.
- Data can only be removed from the end of a stack.

In fact, most computer science literature refers to the end of the stack as its top, and the beginning of the stack as its bottom.

Inserting a new value into a stack is also called pushing onto the stack. Think of it as adding a dish onto the top of the dish stack.

Note that we’re always adding data to the top (that is, the end) of the stack.

because that is the nature of a stack: data can only be added to the top.

Removing elements from the top of the stack is called popping from the stack. Because of a stack’s restrictions, we can only pop data from the top.

A handy acronym used to describe stack operations is LIFO, which stands for “Last In, First Out.” All this means is that the last item pushed onto a stack is always the first item popped from it.

> **Stacks in Action**

**Syntax Error:**

This includes parentheses, square brackets, and curly braces—all common causes of frustrating syntax errors.

Stacks are ideal for processing any data that should be handled in reverse order to how it was received (LIFO).

> Queues

With queues, the first item added to the queue is the first item to be removed.
That’s why computer scientists use the acronym “FIFO”
when it comes to queues: First In, First Out.

Like stacks, queues are arrays with three restrictions (it’s just a different set of restrictions):

- Data can only be inserted at the end of a queue. (This is identical behavior as the stack.)
- Data can only be read from the front of a queue. (This is the opposite of behavior of the stack.)
- Data can only be removed from the front of a queue. (This, too, is the opposite behavior of the stack.)

(While inserting into a stack is called pushing, inserting into a queue doesn’t have a standardized name. Various references call it putting, adding, or enqueuing.)

> Queues in Action

Queues are also the perfect tool for handling asynchronous requests—they ensure that the requests are processed in the order in which they were received.

They are also commonly used to model real-world scenarios where events need to occur in a certain order, such as airplanes waiting for takeoff and patients waiting for their doctor.
