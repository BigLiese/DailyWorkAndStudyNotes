1. 看算法图解到 38 页
   Notes：
   #ARRAY
   1> "Earth-shattering" is an idiomatic expression used to describe something that is extremely important or significant, often implying that it has a large impact or influence. It suggests that something is so significant that it could cause the earth to shake or tremble. For example, "The discovery of a cure for cancer would be an earth-shattering breakthrough in the medical field."

   2> "Flaky" is an adjective used to describe something that is unreliable or inconsistent. It can be used to describe a person, product, or process that fails to work as expected or consistently produces unpredictable or varying results. For example, "The software has been acting flaky lately, crashing unexpectedly and producing inconsistent results.

3> 4 basic ways of using data structures:

- Read
- Search
- Insert
- Delete

> when we measure how “fast” an operation takes, we do not refer to how fast the operation takes in terms of pure time, but instead in how many steps it takes.

1. A computer can jump to any memory address in one step. (Think of this as driving to 123 Main Street—you can drive there in one trip since you know exactly where it is.)
2. Recorded in each array is the memory address which it begins at. So the computer has this starting address readily.
3. Every array begins at index 0.

--> Reading from an array actually takes just _one step_. This is because the computer has the ability to jump to any particular index in the array and peer inside.

--> search operation—in which the computer checks each cell one at a time—is known as _linear search_.

For N cells in an array, linear search will take a maximum of N steps.

--> insertion in a worst-case scenario can take _up to N + 1 steps_ for an array containing N elements. This is because the worst-case scenario is inserting a value into the beginning of the array in which there are N shifts (every data element of the array) and one insertion.

--> delete for an array containing N elements, the maximum number of steps that deletion would take is _N steps_.

---

#SET
A set is a data structure that does not allow duplicate values to be contained within it.

--> Reading from a set is exactly the same as reading from an array—it takes just one step for the computer to look up what is contained within a particular index

-->Searching a set also turns out to be no different than searching an array—it takes up to _N steps_ to search to see if a value exists within a set.

--> Insertion, insertion into a set in a best-case scenario will take _N + 1 steps_ for N elements. This is because there are N steps of search to ensure that the value doesn’t already exist within the set, and then one step for the actual insertion.

In a worst-case scenario, where we’re inserting a value at the beginning of a set, the computer needs to search N cells to ensure that the set doesn’t already contain that value, and then another N steps to shift all the data to the right, and another final step to insert the new value. That’s a total of _2N + 1 steps_.

-->deletion is also identical between a set and an array—it takes up to _N steps_ to delete a value and move data to the left to close the gap.

---

**Chapter 2 Why Algorithms Matter**

An algorithm is simply a particular process for solving a problem.

> Ordered Arrays: the values of the array remain sorted.

when inserting into an ordered array, we need to always conduct _a search before the actual insertion_ to determine the correct spot for the insertion. That is one key difference (in terms of efficiency) between a standard array and an ordered array.

The major advantage of an ordered array over a standard array is that we have the option of performing a binary search rather than a linear search.

> Binary Search: find the midpoint between the upper and lower bounds

First, we establish the lower and upper bounds of where the value we're searching for can be. To start, the lower bound is the first value in the array, while the upper bound is the last value.

When we use binary search, however, each guess we make eliminates half of the possible cells we’d have to search. In our very first guess, we get to eliminate a whopping fifty cells.

[[binary search.png]]

for every time we double the data, the binary search algorithm adds a maximum of just one more step.

> Binary Search vs. Linear Search

With an array containing one hundred values, here are the maximum numbers of steps it would take for each type of search: Linear search: one hundred steps Binary search: seven steps

[[Binary Search vs. Linear Search.png]]

---

**Chapter 3 Oh Yes! Big O Notation：Count the Steps**

Instead of focusing on units of time, Big O achieves consistency by focusing only on the number of steps that an algorithm takes.

Reading from an array takes just one step, no matter how large the array is. The way to express this in Big O Notation is: O(1)

Let’s examine how Big O Notation would describe the efficiency of linear search：
In a worst-case scenario, linear search will take as many steps as there are elements in the array.

The appropriate way to express this in Big O Notation is：O(N) [pronounced as “Oh of N.”]

Big O answers the following question: how does the number of steps change as the data increases?

> Constant Time vs. Linear Time

[[Constant Time vs. Linear Time.png]]

Even though the algorithm technically takes three steps rather than one step, Big O Notation considers that trivial. O(1) is the way to describe any algorithm that doesn’t change its number of steps even when the data increases.

O(N) is considered to be, on the whole, less efficient than O(1).

> Binary search in Big O

In Big O, we describe binary search as having a time complexity of: O(log N)
[pronounced as “Oh of log N.”]
[Log is shorthand for logarithm.]
[Whenever we say O(log N), it’s actually shorthand for saying O(log2 N). We’re just omitting that small 2 for convenience.]

O(log N) is the Big O way of describing an algorithm that increases one step each time the data is doubled.

[[O(logN) O(1) O(N).png]]

[[Logarithms.png]]

---

**Chapter 4 Speeding Up Your Code with Big O**

> Bubble Sort

Given an array of unsorted numbers, how can we sort them so that they end up in ascending order?

If we get through an entire passthrough without having to make any swaps, we’ll know that the array is completely sorted.

The Bubble Sort algorithm contains two kinds of steps:

Comparisons: two numbers are compared with one another to determine which is greater.
Swaps: two numbers are swapped with one another in order to sort them.

For N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.

In Big O Notation, we would say that Bubble Sort has an efficiency of _O(N2 )_.
In an O(N2 ) algorithm, for N data elements, there are roughly _N2 steps._

[[O(N^2).png]]

[O(N2 ) is also referred to as quadratic time]

A common example of an algorithm with quadratic time complexity is a *nested loop*

---

**Chapter5 Optimizing Code with and Without Big O**

> **Selection Sort**

Selection Sort contains two types of steps: comparisons and swaps. That is, we compare each element with the lowest number we’ve encountered in each passthrough, and we swap the lowest number into its correct position.

The steps of Selection Sort are as follows:

We check each cell of the array from left to right to determine which value is least. As we move from cell to cell, we keep in a variable the lowest value we’ve encountered so far.

If we encounter a cell that contains a value that is even less than the one in our variable, we replace it so that the variable now points to the new index.

Once we’ve determined which index contains the lowest value, we swap that index with the value we began the passthrough with. This would be index 0 in the first passthrough, index 1 in the second passthrough, and so on and so forth.

![[Selection Sort.png]]

> **The Efficiency of Selection Sort**

[[The Efficiency of Selection Sort.png]]

Selection Sort contains about half the number of steps that Bubble Sort does, indicating that Selection Sort is twice as fast.

we’d say that for N elements, we make (N - 1) + (N - 2) + (N - 3) … + 1 comparisons.

As for swaps we only need to make a maximum of one swap per passthrough. This is because in each passthrough, we make either one or zero swaps, depending on whether the lowest number of that passthrough is already in the correct position. Contrast this with Bubble Sort, where in a worst-case scenario—an array in descending order—we have to make a swap for each and every comparison.

> **Ignoring Constants**

In reality, Selection Sort is described in Big O as O(N2 ), just like Bubble Sort.

This is because of a major rule of Big O that we’re now introducing for the first time:
_Big O Notation ignores constants._

> **The Role of Big O**

there really is no such thing as O(100N)—it is simply written as O(N).
Similarly, with large amounts of data, O(log N) will always be faster than O(N), even if the given O(log N) algorithm is actually O(2 \* log N) under the hood.

So Big O is an extremely useful tool, because if two algorithms fall under different classifications of Big O, you’ll generally know which algorithm to use since with large amounts of data, one algorithm is guaranteed to be faster than the other at a certain point.

---

**Chapter 6 Optimizing for Optimistic Scenarios**

> **Insertion Sort**

Insertion Sort consists of the following steps:
![[Insertion Sort.png]]
![[Insertion Sort2.png]]

**The Efficiency of Insertion Sort**

There are four types of steps that occur in Insertion Sort: removals, comparisons, shifts, and insertions. To analyze the efficiency of Insertion Sort, we need to tally up each of these steps.

We can, formulate the total number of comparisons as: 1 + 2 + 3 + … + N - 1 comparisons.

When examining this pattern, it emerges that for an array containing N elements, there are approximately N2 / 2 comparisons. (10 2 / 2 is 50, and 20 2 / 2 is 200.)

Let’s add up comparisons and shifts for a worst-case scenario:
N2 / 2 comparisons + N2 / 2 shifts

Removing and inserting the temp_value from the array happen once per passthrough.
N2 comparisons & shifts combined
N - 1 removals + N - 1 insertions

total is _N2 + 2N - 2 steps_
actual is O(N2)

there is another major rule of Big O :
Big O Notation only takes into account the highest order of N.

if we have some algorithm that takes N4 + N3 + N2 + N steps, we only consider N4 to be significant—and just call it O(N4 ).

Why? As N increases, N4 becomes so much more significant than any other order of N. When N is 1,000, N4 is 1,000 times greater than N3 . Because of this, we only consider the greatest order of N.

We noted in the previous chapter that although Bubble Sort and Selection Sort are both O(N2 ), Selection Sort is faster since Selection Sort has N2 / 2 steps compared with Bubble Sort’s N2 steps. At first glance, then, we’d say that Insertion Sort is as slow as Bubble Sort, since it too has N2 steps. (It’s really N2 + 2N - 2 steps.)

> **The Average Case**

The array [1, 2, 3, 4] is already presorted, which is the best case. The worst case for the same data would be [4, 3, 2, 1], and an example of an average case might be [1, 3, 4, 2].

We can now see that the performance of Insertion Sort varies greatly based on the scenario. In the worst-case scenario, Insertion Sort takes N2 steps.
In an average scenario, it takes N2 / 2 steps.
And in the best-case scenario, it takes about N steps.

Selection Sort takes N2 / 2 steps in all cases, from worst to average to best-case scenarios. This is because Selection Sort doesn’t have any mechanism for ending a passthrough early at any point. Each passthrough compares every value to the right of the chosen index no matter what.

So which is better: Selection Sort or Insertion Sort? The answer is: well, it depends. In an average case—where an array is randomly sorted—they perform similarly. If you have reason to assume that you’ll be dealing with data that is mostly sorted, Insertion Sort will be a better choice. If you have reason to assume that you’ll be dealing with data that is mostly sorted in reverse order, Selection Sort will be faster. If you have no idea what the data will be like, that’s essentially an average case, and both will be equal.

> **A Practical Example**

For example, if you have the arrays [3, 1, 4, 2] and [4, 5, 3, 6], the intersection would be a third array, [3, 4], since both of those values are common to the two arrays.

```
function intersection(first_array, second_array){
       var result = [];
       for (var i = 0; i < first_array.length; i++) {
          for (var j = 0; j < second_array.length; j++) {
             if (first_array[i] == second_array[j]) {
                result.push(first_array[i]);
                break;  //as soon as we find a common value we cut the inner loop short, and save steps
                }
             }
         }
              return result;
    }
```

---

**Chapter 7 Blazing Fast Lookup with Hash Tables**

> **Enter the Hash Table : fast reading**

Other names include hashes, maps, hash maps, dictionaries, and associative arrays, Object.

**A hash table is a list of paired values. **
The first item is known as the key, and the second item is known as the value.

Looking up a value in a hash table has an efficiency of O(1) on average, as it takes just one step.

> **Dealing with Collisions**

Trying to add data to a cell that is already filled is known as a collision.

One classic approach for handling collisions is known as separate chaining. When a collision occurs, instead of placing a single value in the cell, it places in it a reference to an array.

[[Dealing with Collisions.png]]

In a scenario where the computer hits upon a cell that references an array, its search can take some extra steps, as it needs to conduct a linear search within an array of multiple values. If somehow all of our data ended up within a single cell of our hash table, our hash table would be no better than an array.

So it actually turns out that the worst-case performance for a hash table lookup is O(N). Because of this, it is critical that a hash table be designed in a way that it will have few collisions, and therefore typically perform lookups in O(1) time rather than O(N) time.

> **The Great Balancing Act**

Ultimately, a hash table’s efficiency depends on three factors:

- How much data we’re storing in the hash table
- How many cells are available in the hash table
- Which hash function we’re using

_A good hash table strikes a balance of avoiding collisions while not consuming lots of memory._

Computer scientists have developed the following rule of thumb: for every seven data elements stored in a hash table, it should have ten cells.

This ratio of data to cells is called the load factor. Using this terminology, we’d say that the ideal load factor is 0.7 (7 elements / 10 cells).

```
function hasDuplicateValue(array) {
      var existingValues = {};
      for(var i = 0; i < array.length; i++) {
         if(existingValues[array[i]] === undefined) {
               existingValues[array[i]] = 1;
               } else {
               return true;
             }
        }
        return false；
    }
```

The truth is that hash tables are perfect for any situation where we want to keep track of which values exist within a dataset.

---

**Chapter 8 Crafting Elegant Code with Stacks and Queues**

> \*\*data structures: stacks and queues --- arrays with restrictions

Stacks and queues are elegant tools for handling temporary data

Temporary data is information that doesn’t have any meaning after it’s processed, so you don’t need to keep it around. However, the order in which you process the data can be important.

Stacks and queues allow you to handle data in order, and then get rid of it once you don’t need it anymore.

> **Stacks**

A stack stores data in the same way that arrays do—it’s simply a list of elements.
The one catch is that stacks have the following three constraints:

- Data can only be inserted at the end of a stack.
- Data can only be read from the end of a stack.
- Data can only be removed from the end of a stack.

In fact, most computer science literature refers to the end of the stack as its top, and the beginning of the stack as its bottom.

Inserting a new value into a stack is also called pushing onto the stack. Think of it as adding a dish onto the top of the dish stack.

Note that we’re always adding data to the top (that is, the end) of the stack.

because that is the nature of a stack: data can only be added to the top.

Removing elements from the top of the stack is called popping from the stack. Because of a stack’s restrictions, we can only pop data from the top.

A handy acronym used to describe stack operations is LIFO, which stands for “Last In, First Out.” All this means is that the last item pushed onto a stack is always the first item popped from it.

> **Stacks in Action**

**Syntax Error:**

This includes parentheses, square brackets, and curly braces—all common causes of frustrating syntax errors.

Stacks are ideal for processing any data that should be handled in reverse order to how it was received (LIFO).

> Queues

With queues, the first item added to the queue is the first item to be removed.
That’s why computer scientists use the acronym “FIFO”
when it comes to queues: First In, First Out.

Like stacks, queues are arrays with three restrictions (it’s just a different set of restrictions):

- Data can only be inserted at the end of a queue. (This is identical behavior as the stack.)
- Data can only be read from the front of a queue. (This is the opposite of behavior of the stack.)
- Data can only be removed from the front of a queue. (This, too, is the opposite behavior of the stack.)

(While inserting into a stack is called pushing, inserting into a queue doesn’t have a standardized name. Various references call it putting, adding, or enqueuing.)

> Queues in Action

Queues are also the perfect tool for handling asynchronous requests—they ensure that the requests are processed in the order in which they were received.

They are also commonly used to model real-world scenarios where events need to occur in a certain order, such as airplanes waiting for takeoff and patients waiting for their doctor.

---

**Chapter 9 Recursively Recurse with Recursion**

> **Recursion 递归**

Recursion is the official name for when a function calls itself.

Recursion allows for solving tricky problems in surprisingly simple ways, often allowing us to write a fraction of the code that we might otherwise write.

> **Recurse Instead of Loop**

```
function countdown(number) {
       for(var i = number; i >= 0; i--) {
       console.log(i);
       }
    }
    countdown(10);
```

```
function countdown(number) {
console.log(number);
countdown(number - 1);
 }
  countdown(10);
```

We’re not using any loop constructs, but by simply having the countdown function call itself, we are able to count down from 10 and print each number to the console.

In almost any case in which you can use a loop, you can also use recursion.

Recursion is a tool that allows for writing elegant code.

our solution is not perfect, as we’ll end up stuck with infinitely printing negative numbers.

> **Base Case**

We can solve this problem by adding a conditional statement that ensures that if number is currently 0, we don’t call countdown() again:

```
function countdown(number) {
    console.log(number);
      if(number === 0) {
         return;
         } else {
         countdown(number - 1);
         }
        }
        countdown(10);
```

In Recursion Land (a real place), this case in which the method will not recurse is known as the base case. So in our countdown() function, 0 is the base case.

> **Reading Recursive Code**

Let’s look at another example: calculating factorials (阶乘) .

A factorial is best illustrated with an example:

The factorial of 3 is: 3 _ 2 _ 1 = 6
The factorial of 5 is: 5 _ 4 _ 3 _ 2 _ 1 = 120

> **Recursion in the Eyes of the Computer**

The computer calls factorial(3), and before the method is complete, it calls factorial(2), and before factorial(2) is complete, it calls factorial(1). Technically, while the computer runs factorial(1), it’s still in the middle of factorial(2), which in turn is running within factorial(3).

The computer uses a stack to keep track of which functions it’s in the middle of calling.
This stack is known, appropriately enough, as the **call stack.**

Looking back at this example from a bird’s-eye view, you’ll see that the order in which the computer calculates the factorial of 3 is as follows: 1. factorial(3) is called first. 2. factorial(2) is called second. 3. factorial(1) is called third. 4. factorial(1) is completed first. 5. factorial(2) is completed based on the result of factorial(1). 6. Finally, factorial(3) is completed based on the result of factorial(2).

Interestingly, in the case of infinite recursion (such as the very first example in our chapter), the program keeps on pushing the same method over and over again onto the call stack, until there’s no more room in the computer’s memory —and this causes an error known as stack overflow.

---

**Chapter 10 Recursive Algorithms for Speed**

In previous chapters, we’ve encountered a number of sorting algorithms, including Bubble Sort, Selection Sort, and Insertion Sort. In real life, however, none of these methods are actually used to sort arrays. Most computer languages have built-in sorting functions for arrays that save us the time and effort from implementing our own. And in many of these languages, the sorting algorithm that is employed under the hood is Quicksort.

Quicksort is an extremely fast sorting algorithm that is particularly efficient for average scenarios. While in worst-case scenarios (that is, inversely sorted arrays), it performs similarly to Insertion Sort and Selection Sort, it is much faster for average scenarios—which are what occur most of the time.

Quicksort relies on a concept called partitioning(分割), so we’ll jump into that first. (分割)

To partition an array is to take a random value from the array—which is then called the pivot—and make sure that every number that is less than the pivot ends up to the left of the pivot, and that every number that is greater than the pivot ends up to the right of the pivot. We accomplish partitioning through a simple algorithm that will be described in the following example.

![[pics/patrition.png]]

1. The left pointer continuously moves one cell to the right until it reaches a value that is greater than or equal to the pivot, and then stops.
2. Then, the right pointer continuously moves one cell to the left until it reaches a value that is less than or equal to the pivot, and then stops.
3. We swap the values that the left and right pointers are pointing to.
4. We continue this process until the pointers are pointing to the very same value or the left pointer has moved to the right of the right pointer.
5. Finally, we swap the pivot with the value that the left pointer is currently pointing to.

When we’re done with a partition, we are now assured that all values to the left of the pivot are less than the pivot, and all values to the right of the pivot are greater than it. And that means that the pivot itself is now in its correct place within the array, although the other values are not yet necessarily completely sorted.

![[partrition example step1.png]]

Step #3: Compare the right pointer (6) to our pivot. Is the value greater than the pivot? It is, so our pointer will move on in the next step.

Step #4: The right pointer moves on:

![[partrition example step2.png]]

page 190

> **Quicksort**

The Quicksort algorithm relies heavily on partitions. It works as follows:

1. Partition the array. The pivot is now in its proper place.
2. Treat the subarrays to the left and right of the pivot as their own arrays, and recursively repeat steps #1 and #2. That means that we’ll partition each subarray, and end up with even smaller subarrays to the left and right of each subarray’s pivot. We then partition those subarrays, and so on and so forth.
3. When we have a subarray that has zero or one elements, that is our base case and we do nothing.

> **The Efficiency of Quicksort**

a partition consists of two types of steps:

- Comparisons: We compare each value to the pivot.
- Swaps: When appropriate, we swap the values being pointed to by the left and right pointers.

Each partition has at least N comparisons—that is, we compare each element of the array with the pivot.

The number of swaps depends on how the data is sorted. Each partition has at least one swap, and the most swaps that a partition can have would be N / 2

Now, for randomly sorted data, there would be roughly half of N / 2 swaps, or N / 4 swaps.

So with N comparisons and N / 4, we’d say there are about 1.25N steps. In Big O Notation, we ignore constants, so we’d say that a partition runs in _O(N) time._

In a worst-case scenario, Quicksort has an efficiency of O(N2 )

> **Quickselect**

Quickselect relies on partitioning just like Quicksort, and can be thought of as a hybrid of Quicksort and binary search.

Quickselect is that we can find the correct value without having to sort the entire array.

With Quicksort, for every time we cut the array in half, we need to re-partition every single cell from the original array, giving us O(N log N). With Quickselect, on the other hand, for every time we cut the array in half, we only need to partition the one half that we care about—the half in which we know our value is to be found.

When analyzing the efficiency of Quickselect, we’ll see that it’s O(N) for average scenarios.
Quickselect has an efficiency of _O(N)._

---

**Chapter 11 Node-Based Data Structures**

> **Linked Lists**

A linked list is a data structure that represents a list of items, just like an array. In fact, in any application in which you’re using an array, you could probably use a linked list instead.

Linked lists, do not consist of a bunch of memory cells in a row. Instead, they consist of a bunch of memory cells that are not next to each other, but can be spread across many different cells across the computer’s memory.

These cells that are not adjacent to each other are known as nodes.

![[Linked Lists.png]]

> **Reading**

We noted previously that when reading a value from an array, the computer can jump to the appropriate cell in a single step, which is O(1). This is not the case, however, with a linked list.

all the program knows is the memory address of the first node of the linked list. To find index 2 (which is the third node), the program must begin looking up index 0 of the linked list, and then follow the link at index 0 to index 1. It must then again follow the link at index 1 to index 2, and finally inspect the value at index 2.

Reading a linked list has an efficiency of O(N). This is a significant disadvantage in comparison with arrays in which reads are O(1).

> **Searching**

Arrays and linked lists have the same efficiency for search.

Remember, searching is looking for a particular piece of data within the list and getting its index. With both arrays and linked lists, the program needs to start at the first cell and look through each and every cell until it finds the value it’s searching for. In a worst-case scenario—where the value we’re searching for is in the final cell or not in the list altogether—it would take _O(N) steps_

> **Insertion**

Insertion is one operation in which linked lists can have a distinct advantage over arrays in certain situations.

inserting into the middle of a linked list takes O(N), just as it does for an array.

however, insertion at the beginning of the list takes just one step—which is O(1).

[[Linked list vs Array.png]]

> **Deletion**

Deletion is very similar to insertion in terms of efficiency. To delete a node from the beginning of a linked list, all we need to do is perform one step: we change the first_node of the linked list to now point to the second node.

Contrast this with an array in which deleting the first element means shifting all remaining data one cell to the left, an efficiency of O(N).

When it comes to deleting the final node of a linked list, the actual deletion takes one step—we just take the second-to-last node and make its link null. However, it takes N steps to first get to the second-to-last node, since we need to start at the beginning of the list and follow the links until we reach it.

![[Linked List steps.png]]

> \*\*Linked Lists in Action

One case where linked lists shine is when we examine a single list and delete many elements from it.

With an array, each time we delete an email address, we need another O(N) steps to shift the remaining data to the left to close the gap. All this shifting will happen before we can even inspect the next email address.

So besides the N steps of reading each email address, we need another N steps multiplied by invalid email addresses to account for deletion of invalid email addresses.

Let’s assume that one in ten email addresses are invalid. If we had a list of 1,000 email addresses, there would be approximately 100 invalid ones, and our algorithm would take 1,000 steps for reading plus about 100,000 steps for deletion (100 invalid email addresses \* N).

With a linked list, however, as we comb through the list, each deletion takes just one step, as we can simply change a node’s link to point to the appropriate node and move on.
For our 1,000 emails, then, our algorithm would take just 1,100 steps, as there are 1,000 reading steps, and 100 deletion steps.

> **Doubly Linked List**

A doubly linked list is like a linked list, except that each node has two links— one that points to the next node, and one that points to the preceding node. In addition, the doubly linked list keeps track of both the first and last nodes.

Because doubly linked lists have immediate access to both the front and end of the list, they can insert data on either side at O(1) as well as delete data on either side at O(1). Since doubly linked lists can insert data at the end in O(1) time and delete data from the front in O(1) time, they make the perfect underlying data structure for a queue.
